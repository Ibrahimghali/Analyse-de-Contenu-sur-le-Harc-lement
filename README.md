
---

# Plateforme dâ€™Analyse de Contenu sur le HarcÃ¨lement

Un pipeline complet pour collecter, traiter et analyser les contenus liÃ©s au harcÃ¨lement sur les rÃ©seaux sociaux.

## ğŸ“‹ Table des matiÃ¨res

* [PrÃ©sentation gÃ©nÃ©rale](#prÃ©sentation-gÃ©nÃ©rale)
* [Architecture du projet](#architecture-du-projet)
* [Ã‰tapes du pipeline](#Ã©tapes-du-pipeline)
* [Choix techniques](#choix-techniques)
* [PrÃ©requis](#prÃ©requis)
* [Installation](#installation)

## PrÃ©sentation gÃ©nÃ©rale

Ce projet propose une solution complÃ¨te pour analyser le harcÃ¨lement sur diverses plateformes sociales. Il collecte des donnÃ©es depuis Twitter, Reddit et Telegram, applique des traitements NLP (traitement du langage naturel) pour enrichir les contenus, puis les indexe dans Elasticsearch afin dâ€™offrir des capacitÃ©s avancÃ©es de recherche et de visualisation.

## Architecture du projet

```
Analyse-de-Contenu-sur-le-Harc-lement/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main/
â”‚   â”‚   â”œâ”€â”€ scrapers/           # Modules de collecte
â”‚   â”‚   â”‚   â”œâ”€â”€ twitter_scraper.py
â”‚   â”‚   â”‚   â”œâ”€â”€ reddit_scraper.py
â”‚   â”‚   â”‚   â”œâ”€â”€ telegram_scraper.py
â”‚   â”‚   â”‚   â””â”€â”€ scraper.py      # Orchestration des scrapers
â”‚   â”‚   â”œâ”€â”€ processeing/        # Modules de traitement
â”‚   â”‚   â”‚   â”œâ”€â”€ preprocessing.py  # Nettoyage et normalisation
â”‚   â”‚   â”‚   â””â”€â”€ nlp_pipeline.py   # DÃ©tection langue & sentiment
â”‚   â”‚   â”œâ”€â”€ migration/          # Migration de donnÃ©es
â”‚   â”‚   â”‚   â””â”€â”€ es_ingest.py    # Indexation Elasticsearch
â”‚   â”‚   â””â”€â”€ main.py             # Orchestrateur principal
â”‚   â””â”€â”€ test/                   # Tests unitaires
â”‚       â”œâ”€â”€ test_scraper.py
â”‚       â”œâ”€â”€ test_preprocessing.py
â”‚       â””â”€â”€ test_nlp_pipeline.py
â”œâ”€â”€ logs/                       # Journaux d'application
â”œâ”€â”€ data/                       # Stockage des donnÃ©es
â”œâ”€â”€ Dockerfile                  # Conteneurisation
â”œâ”€â”€ docker-compose.yaml         # Orchestration multi-conteneurs
â””â”€â”€ requirements.txt            # DÃ©pendances Python
```

## Ã‰tapes du pipeline

### 1. Collecte de donnÃ©es

* Extraire des contenus liÃ©s au harcÃ¨lement sur Twitter (API v2), Reddit (subreddits ciblÃ©s) et Telegram (groupes publics).
* Stocker les donnÃ©es brutes dans MongoDB (`posts`).

### 2. PrÃ©traitement

* **Nettoyage multi-Ã©tapes** :
  * Suppression des balises HTML via expressions rÃ©guliÃ¨res
  * Ã‰limination des URLs (http, https, www)
  * Retrait des caractÃ¨res spÃ©ciaux, de la ponctuation et des chiffres
  * Conversion en minuscules pour uniformisation
* **Traitement linguistique** :
  * Tokenisation du texte (dÃ©coupage en mots individuels)
  * Filtrage des stopwords en franÃ§ais ET en anglais
  * Ã‰limination des mots trop courts (< 3 caractÃ¨res)
  * Lemmatisation via NLTK WordNetLemmatizer (rÃ©duction Ã  la forme canonique)
* **Traitement par lots** :
  * Suivi de progression en temps rÃ©el (logs tous les 100 documents)
  * Traitement sÃ©parÃ© du titre et du contenu
  * Conservation des mÃ©tadonnÃ©es (auteur, date, URL)
* **Persistance optimisÃ©e** :
  * Sauvegarde dans MongoDB avec indexation sur URL
  * MÃ©canisme d'upsert pour Ã©viter les doublons
  * Structure document original + version prÃ©traitÃ©e

### 3. Traitement NLP

* **DÃ©tection de langue** :
  * Utilisation de la bibliothÃ¨que LangDetect
  * Analyse des 1000 premiers caractÃ¨res pour efficacitÃ©
  * Gestion des cas spÃ©ciaux (textes courts, erreurs de dÃ©tection)
  * Support multilingue avec prioritÃ© franÃ§ais/anglais
* **Analyse de sentiment** :
  * Utilisation de TextBlob avec adaptation multilingue
  * Calcul de polaritÃ© sur Ã©chelle normalisÃ©e (-1 Ã  1)
  * Classification en trois catÃ©gories :
    * Positif (polaritÃ© > 0.1)
    * NÃ©gatif (polaritÃ© < -0.1)
    * Neutre (-0.1 â‰¤ polaritÃ© â‰¤ 0.1)
  * Prise en compte du contexte linguistique
* **Enrichissement des donnÃ©es** :
  * Ajout des champs `langue` et `sentiment` Ã  chaque document
  * Conservation du score numÃ©rique de polaritÃ© pour analyses fines
  * Horodatage du traitement (`enriched_at`)
* **IntÃ©gration et extensibilitÃ©** :
  * SystÃ¨me de traitement modulaire
  * PossibilitÃ© d'export vers CSV pour analyses externes
  * Architecture permettant l'ajout de nouvelles analyses (entitÃ©s nommÃ©es, classification, etc.)

### 4. Indexation Elasticsearch

* CrÃ©er un index `harcelement_posts` avec mapping adaptÃ©.
* Transformer et indexer les documents par lots.
* Indexer les champs : titre, contenu, auteur, date, URL, langue, sentiment, score.

### 5. Visualisation avec Kibana

* Explorer via dashboards interactifs.
* Visualiser la rÃ©partition des sentiments.
* Effectuer des recherches avancÃ©es.
* GÃ©nÃ©rer des rapports sur les tendances du harcÃ¨lement.

## Choix techniques

### Langages & outils

* **Python 3.9** : riche Ã©cosystÃ¨me NLP.
* **MongoDB** : flexible pour donnÃ©es non structurÃ©es.
* **Elasticsearch & Kibana** : recherche et visualisation avancÃ©es.

### BibliothÃ¨ques principales

* Tweepy (Twitter API), AsyncPRAW (Reddit), Telethon (Telegram)
* NLTK, TextBlob, LangDetect pour NLP.

### Conteneurisation

* Docker & Docker Compose pour dÃ©ploiement cohÃ©rent.

### Architecture

* Modulaire et asynchrone pour efficacitÃ©.
* Journalisation Ã©tendue pour dÃ©bogage.

### Mapping Elasticsearch

* Champs texte avec sous-champs `.keyword` pour agrÃ©gation.
* Champs `keyword` pour auteur, langue, sentiment.
* Analyseur standard multilingue.

## PrÃ©requis

* Python 3.9+
* Docker & Docker Compose
* ClÃ©s API Twitter, Reddit, Telegram

## Installation 

1. Cloner le dÃ©pÃ´t :

```bash
git clone https://github.com/votrenomdutilisateur/Smart-Conseil.git
cd Smart-Conseil
```

2. CrÃ©er un fichier `.env` avec vos identifiants API :

```
TWITTER_BEARER_TOKEN=...
REDDIT_CLIENT_ID=...
REDDIT_CLIENT_SECRET=...
TELEGRAM_API_ID=...
TELEGRAM_API_HASH=...
TELEGRAM_PHONE=...
```

3. Construire et lancer les conteneurs :

```bash
docker-compose up -d
```

---

